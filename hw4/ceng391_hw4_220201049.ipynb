{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "from numpy.linalg import inv\n",
    "\n",
    "def transformPoints(t):   #If it is inlier then return 1, else return 0.This func for counting\n",
    "    xPrime=float(t[0]/t[2])\n",
    "    yPrime=float(t[1]/t[2])\n",
    "    \n",
    "    for index in range(len(dst_points)):\n",
    "        if (math.sqrt((xPrime - dst_points[index][0])**2 + (yPrime - dst_points[index][1])**2 ) <= 3.0):\n",
    "            return 1                \n",
    "    return 0      \n",
    "\n",
    "\n",
    "def random4points(): #Find randomly 4 points from the matches keypoints\n",
    "    random=np.random.randint(0,len(src_points))\n",
    "    random2=np.random.randint(0,len(src_points))\n",
    "    random3=np.random.randint(0,len(src_points))\n",
    "    random4=np.random.randint(0,len(src_points))\n",
    "    \n",
    "    src_random=[]\n",
    "    dst_random=[]\n",
    "    \n",
    "    src_random.append((src_points[random]))\n",
    "    src_random.append((src_points[random2]))\n",
    "    src_random.append((src_points[random3]))\n",
    "    src_random.append((src_points[random4]))\n",
    "    \n",
    "    dst_random.append((dst_points[random]))\n",
    "    dst_random.append((dst_points[random2]))\n",
    "    dst_random.append((dst_points[random3]))\n",
    "    dst_random.append((dst_points[random4]))\n",
    "\n",
    "    return np.array(src_random),np.array(dst_random)\n",
    "\n",
    "def updateRansac(inlier): #TO update Ransac N value\n",
    "    \n",
    "    w=float(inlier/ len (src_points))\n",
    "    ransac= -2.0 / float(math.log10 ( 1.0 - (w)**4 ))\n",
    "    return int(ransac - 1)\n",
    "\n",
    "def dlt(src_points,dst_points):  #Direct Linear Translation function returns homography\n",
    "    average_src_x =0\n",
    "    average_src_y =0\n",
    "    average_dst_x =0\n",
    "    average_dst_y =0\n",
    "   \n",
    "    for i in range(len(src_points)):\n",
    "        average_src_x+=src_points[i][0]\n",
    "        average_src_y+=src_points[i][1]\n",
    "        average_dst_x+=dst_points[i][0]\n",
    "        average_dst_y+=dst_points[i][1]\n",
    "    \n",
    "    average_src_x= float(average_src_x / len(src_points))\n",
    "    average_src_y= float(average_src_y / len(src_points))\n",
    "    average_dst_x= float(average_dst_x / len(src_points))\n",
    "    average_dst_y= float(average_dst_y / len(src_points))\n",
    "\n",
    "    new_src_points=[]\n",
    "    new_dst_points=[]\n",
    " \n",
    "    for i in range(len(src_points)):\n",
    "\n",
    "        x1=src_points[i][0]-average_src_x\n",
    "        y1=src_points[i][1]-average_src_y\n",
    "        x2=dst_points[i][0]-average_dst_x\n",
    "        y2=dst_points[i][1]-average_dst_y\n",
    "    \n",
    "        new_src_points.append([x1,y1])\n",
    "        new_dst_points.append([x2,y2])\n",
    "    \n",
    "    distancesrc=0\n",
    "    distancedst=0\n",
    "    \n",
    "    for i in range(len(new_src_points)):\n",
    "        distancesrc += float(math.sqrt(new_src_points[i][0]**2 +   new_src_points[i][1]**2))\n",
    "        distancedst += float(math.sqrt(new_dst_points[i][0]**2 +   new_dst_points[i][1]**2))\n",
    "  \n",
    "    lamda=  float( math.sqrt(2)/(distancesrc))\n",
    "    lamda2=  float( math.sqrt(2)/(distancedst))\n",
    "   \n",
    "    t=np.array([[float(lamda),0,float(lamda)*(-average_src_x)],[0,float(lamda),float(lamda)*(-average_src_y)],[0,0,1]])\n",
    "    t2=np.array([[float(lamda2),0,float(lamda2)*(-average_dst_x)],[0,float(lamda2),float(lamda2)*(-average_dst_y)],[0,0,1]])\n",
    "    listA=[]\n",
    "    \n",
    "    for i in range(len(new_src_points)):\n",
    "        new_src_points[i][0]= lamda * new_src_points[i][0]\n",
    "        new_src_points[i][1]= lamda * new_src_points[i][1]\n",
    "        new_dst_points[i][0]= lamda2 * new_dst_points[i][0]\n",
    "        new_dst_points[i][1]= lamda2 * new_dst_points[i][1]      \n",
    "    \n",
    "    for i in range(len(src_points)):\n",
    "        temp = np.dot (t,np.array([[src_points[i][0]],[src_points[i][1]],[1.0]]))\n",
    "        \n",
    "        temp[0] = temp[0] /temp[2]\n",
    "        temp[1] = temp[1] /temp[2]\n",
    "        temp=np.array(temp)\n",
    "     \n",
    "        temp2 =  np.dot (t2,np.array([[dst_points[i][0]],[dst_points[i][1]],[1.0]]))\n",
    "        temp2[0] = temp2[0] /temp2[2]\n",
    "        temp2[1] = temp2[1] /temp2[2]\n",
    "        \n",
    "        temp2=np.array(temp2)\n",
    "        new_src_points[i][0]= temp[0]\n",
    "        new_src_points[i][1]= temp[1]\n",
    "        \n",
    "        \n",
    "        new_dst_points[i][0]= temp2[0]\n",
    "        new_dst_points[i][1]= temp2[1]\n",
    "    \n",
    "    new_src_points=np.array(new_src_points)\n",
    "    new_dst_points=np.array(new_dst_points)\n",
    "    \n",
    "    for i in range(len(new_src_points)):\n",
    "        listA.append([0,0,0,-new_src_points[i][0],-new_src_points[i][1],-1.0,new_dst_points[i][1]*new_src_points[i][0],new_dst_points[i][1]*new_src_points[i][1],new_dst_points[i][1]])\n",
    "        listA.append([new_src_points[i][0],new_src_points[i][1],1.0,0.0,0.0,0.0,-new_dst_points[i][0]*new_src_points[i][0],-new_src_points[i][1]*new_dst_points[i][0],-new_dst_points[i][0]])\n",
    "    listA=np.array(listA)\n",
    "\n",
    "    w,u,vt=cv2.SVDecomp(np.asarray(listA))\n",
    "    homography=[[vt[-1][0],vt[-1][1],vt[-1][2]],[vt[-1][3],vt[-1][4],vt[-1][5]],[vt[-1][6],vt[-1][7],vt[-1][8]]]\n",
    "    invT = inv(t2)\n",
    "    newHomograpy=np.dot(invT,np.array(homography))\n",
    "    final=np.dot(np.array(newHomograpy),t)\n",
    "    return np.array(final)\n",
    "\n",
    "\n",
    "def findInlier(lastHomography,src_points,dst_points): #Find nearest inlier to the point, append to new list and return\n",
    "    listFinalsrc=[]\n",
    "    listFinaldst=[]\n",
    "    indeks=0\n",
    "    x=0\n",
    "    for p1 in src_points:\n",
    "        y=0\n",
    "        flag=False\n",
    "        lowest=3.0\n",
    "        p=np.dot(lastHomography,np.array([[p1[0]],[p1[1]],[1.0]]))\n",
    "        xp=p[0]/p[2]\n",
    "        yp=p[1]/p[2]\n",
    "        \n",
    "        for p2 in dst_points:\n",
    "            t=float(math.sqrt((xp[0] - p2[0])**2 + (yp[0] - p2[1])**2 ))\n",
    "            if (t <= 3.0):\n",
    "                if (lowest>t):\n",
    "                    lowest=t\n",
    "                    indeks=y\n",
    "                    flag=True  \n",
    "            y+=1\n",
    "        if(flag==True):\n",
    "            listFinalsrc.append(p1)\n",
    "            listFinaldst.append(dst_points[indeks])\n",
    "        x+=1\n",
    "    return  np.array(listFinalsrc),np.array(listFinaldst)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_points=[]\n",
    "dst_points=[]\n",
    "img1=cv2.imread(\"img1.jpg\")\n",
    "img2 = cv2.imread('img2.jpg')\n",
    "\n",
    "# trainImage\n",
    "# Initiate SIFT detector\n",
    "sift = cv2.xfeatures2d.SIFT_create(contrastThreshold = 0.04 ,edgeThreshold = 10)\n",
    "# find the keypoints and descriptors with SIFT\n",
    "kp1, des1 = sift.detectAndCompute(img1,None)\n",
    "kp2, des2 = sift.detectAndCompute(img2,None)\n",
    "\n",
    "\n",
    "temp=0   \n",
    "for i in range(len(des1)):               #Match them by using nearest neighbour\n",
    "    distance=100000000\n",
    "\n",
    "    temp=0\n",
    "    for y in range(len(des2)):\n",
    "        x=0\n",
    "        for a in range(len(des1[0])):\n",
    "            x+=math.sqrt((des1[i][a]-des2[y][a])**2) \n",
    "          \n",
    "        if(x<distance):\n",
    "            distance=x\n",
    "            temp=int(y)\n",
    "    src_points.append(kp1[i].pt)    #Our correspondences in src and dst points, It takes time to calculate\n",
    "    dst_points.append(kp2[temp].pt)\n",
    "    \n",
    "print (\"KEYPOINTS\",len(src_points))\n",
    "Nransac=10000  \n",
    "i=0\n",
    "beforeinlier=0\n",
    "\n",
    "while(i<Nransac):    #Starting of RANSAC\n",
    "    inlier=0\n",
    "    inlierRatio=0\n",
    "    src,dst=random4points()   # 4 random point\n",
    "    homography=dlt(src,dst)   #Estimating Homography\n",
    "   \n",
    "    for points in (src_points):     #IN every src_point multiplied by homography and checks whether in dst list is there a pixel distance < 3.\n",
    "    \n",
    "        t=np.dot(homography,np.array([[points[0]],[points[1]],[1.0]]))\n",
    "        inlier+=transformPoints(t)\n",
    "      \n",
    "    inlierRatio= float(inlier / len(src_points)) #Inlier Ratio\n",
    "    if (inlier>=beforeinlier):\n",
    "        Nransac = updateRansac(inlier) #By giving number of inlier, we update N value\n",
    "        beforeinlier=inlier #This is for the copy operation to check inlier's\n",
    "        i=0   #Just reset the loop whenever N updates\n",
    "        lastHomography=np.copy(homography) #Save the last and best homography\n",
    "        print (\"Number of inlier is\",inlier)\n",
    "        print (\"Ransac\",Nransac)\n",
    "        print (\"Inlier ratio is \",inlierRatio * 100)\n",
    "        \n",
    "    \n",
    "    print (\"Loading  \",(i/Nransac)*100)\n",
    "    i+=1\n",
    "\n",
    "print (\"LAST OF LAST homography\",lastHomography) #Best homography yet\n",
    "\n",
    "l1,l2=findInlier(lastHomography,src_points,dst_points) #Find inlier with the best homography.\n",
    "\n",
    "\n",
    "copy=0\n",
    "i=0 #Just find better homography 5 times by inliers\n",
    "while(i<5):   #This should be infinite loop until it converges but our list will be full of same points. I could not understand why ??\n",
    "    print (\"l1\",len(l1))\n",
    "    print (\"l2\",len(l2))\n",
    "    copy=len(l1)\n",
    "    lH=dlt(l1,l2)#Update to estimate better homography\n",
    " \n",
    "    print (\"Before INLIER \",len(l1))\n",
    "    l1,l2=findInlier(lH,l1,l2) #To get better and realistic inlier until it converges.\n",
    "    print (\"After INLIER \",len(l1))\n",
    "    print (copy,len(l1))\n",
    "    if copy==(len(l1)):\n",
    "        break\n",
    "   \n",
    "    i+=1\n",
    "\n",
    "    \n",
    "\n",
    "topLeft=np.dot(lH,np.array([[160.0],[160.0],[1.0]]))\n",
    "\n",
    "topLeft[0]=float(topLeft[0]/topLeft[2])\n",
    "topLeft[1]=float(topLeft[1]/topLeft[2])\n",
    "\n",
    "\n",
    "botRight=np.dot(lH,np.array([[455.0],[576.0],[1.0]]))\n",
    "\n",
    "botRight[0]=float(botRight[0]/botRight[2])\n",
    "botRight[1]=float(botRight[1]/botRight[2])\n",
    "\n",
    "cv2.rectangle(img2,(topLeft[0],topLeft[1]),(botRight[0],botRight[1]),(0,0,255),2)\n",
    "\n",
    "cv2.imwrite(\"object_detection.png\",img2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### COMMENT 1 ####\n",
    "\n",
    "\n",
    "\n",
    "#ORB \n",
    "#ORB uses by the FAST key point detector and BRIEF  descriptor\n",
    "#Then a Harris corner measure is  applied  to  find  top  N  points.    FAST  does  not  compute  the \n",
    "#orientation  and  is  rotation  variant.  It  computes  the  intensity weighted  centroid  \n",
    "#of  the  patch  with  located  corner  at  center. \n",
    "\n",
    "#SIFT algorithm uses Difference of Gaussians\n",
    "\n",
    "#SIFT  proposed  by  Lowe  solves  the  image  rotation,  affine  transformations,  intensity,  and  viewpoint  change  in  matching \n",
    "#features.A key point localization where the key  point  candidates  are  localized  and  refined  by  eliminating \n",
    "\n",
    "#Descriptor to compute the local image descriptor for each   key   point   based   on   image   gradient   magnitude   and orientation \n",
    "\n",
    "#I understood that;\n",
    " \n",
    "#BRIEF performs poorly with rotation.\n",
    "\n",
    "#ORB is the fastest but SIFT is more succes rate. --> Comparing the images with varying intensity or\n",
    "#comparing the image with its rotated image\n",
    "\n",
    "#With  scaled image ORB is faster and more succesfull than SIFT\n",
    "\n",
    "\n",
    "# As I understood briefly from the source:\n",
    "# https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_orb/py_orb.html\n",
    "# https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_sift_intro/py_sift_intro.html?highlight=sift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### COMMENT 2 ####\n",
    "# In SIFT we use Euclidean distance. Every keypoint has length of 128 size\n",
    "#there will be Keypoint x 128. We calculate as follow\n",
    "# (x0-x'0)**2 + (x1-x'1) +....+ x128-x'128) and square root of them\n",
    "\n",
    "#In ORB we use Hamming distance. We use XOR operation to catch the difference\n",
    "#So, between 2 keypoint distance we count (1 values )the differences between keypoints that we gather by XOR operation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### COMMENT 3 ####\n",
    "# Our points of set will be decreased because our distance will lower than 3.0 pixel distance. We can think it as a circle\n",
    "# radius and our point number will decreased since the radius is decreased but the results could be more realistic.\n",
    "# if we decrease too much than our set of points  decreased too much and no longer to find good matching points\n",
    "\n",
    "#If we decrease our number of inlier will decrease but we use nearest neighbour so somepoints will remain the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### COMMENT 4 ####\n",
    "# Normalization is basically putting both points on the border which makes the solution more stable\n",
    "#It's a matter of numerical accuracy. By normalizing the data set, we center data and give it unit variance.\n",
    "#We move the image center to the (0,0)\n",
    "#Normalization is essential not only for numerical stability, but also for more accurate estimation in presence of noise and faster solution\n",
    "#Normalization affects  SVD computation.It is better and to obtain more accurate estimate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
